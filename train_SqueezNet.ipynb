{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f955bf69-fcff-4057-96d2-e31c3b2195fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import utils\n",
    "from fer import FER2013\n",
    "from torch.autograd import Variable\n",
    "from models import *\n",
    "from torchvision import models\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0850d44b-f568-4f61-a701-d0ecac4b7ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\FER-2013 Project\\facialexpressionrecogination\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "F:\\FER-2013 Project\\facialexpressionrecogination\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.squeezenet1_1(pretrained=True)\n",
    "model.classifier[1] = nn.Conv2d(512, 7, kernel_size=(1,1), stride=(1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e32368c-a7bb-4b00-aa45-1ae12143130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.model = 'SqueezeNet'  \n",
    "        self.dataset = 'FER2013'\n",
    "        self.bs = 64  \n",
    "        self.lr = 0.001  \n",
    "        self.resume = True \n",
    "\n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b56484-a698-45d6-817b-292e4f195ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "best_Testing_acc = 0 \n",
    "best_Testing_acc_epoch = 0\n",
    "best_Validation_acc = 0 \n",
    "best_Validation_acc_epoch = 0\n",
    "start_epoch = 0  \n",
    "\n",
    "learning_rate_decay_start = 50  \n",
    "learning_rate_decay_every = 10\n",
    "learning_rate_decay_rate = 0.5  \n",
    "\n",
    "cut_size = 44\n",
    "total_epoch = 100\n",
    "\n",
    "path = os.path.join(opt.dataset + '_' + opt.model)\n",
    "\n",
    "# Lists to save metrics\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "testing_acc_list = []\n",
    "testing_loss_list = []\n",
    "validation_acc_list = []\n",
    "validation_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e49e7caa-2722-4d14-a757-86866c4666da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(cut_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "trainset = FER2013(split='Training', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.bs, shuffle=True, num_workers=0)\n",
    "Testingset = FER2013(split='Testing', transform=transform_test)\n",
    "Testingloader = torch.utils.data.DataLoader(Testingset, batch_size=opt.bs, shuffle=False, num_workers=0)\n",
    "Validationset = FER2013(split='Validation', transform=transform_test)\n",
    "Validationloader = torch.utils.data.DataLoader(Validationset, batch_size=opt.bs, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e182ea04-0d3a-4d16-a7aa-98c05c5b3731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "No checkpoint directory found. Starting training from scratch.\n",
      "==> Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')\n",
    "net = models.squeezenet1_1(pretrained=True)  \n",
    "net.classifier[1] = nn.Conv2d(512, 7, kernel_size=(1, 1), stride=(1, 1))\n",
    "net.num_classes = 7\n",
    "\n",
    "# Initialize weights of the new layer with better values\n",
    "torch.nn.init.kaiming_normal_(net.classifier[1].weight)\n",
    "net.classifier[1].bias.data.zero_()\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    print(\"No checkpoint directory found. Starting training from scratch.\")\n",
    "    opt.resume = False\n",
    "\n",
    "if opt.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = torch.load(os.path.join(path, 'Validation_model.t7'), map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_Testing_acc = checkpoint['best_Testing_acc']\n",
    "    best_Validation_acc = checkpoint['best_Validation_acc']\n",
    "    best_Testing_acc_epoch = checkpoint['best_Testing_acc_epoch']\n",
    "    best_Validation_acc_epoch = checkpoint['best_Validation_acc_epoch']\n",
    "    start_epoch = checkpoint['best_Validation_acc_epoch'] + 1\n",
    "else:\n",
    "    print('==> Starting training from scratch.')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=opt.lr, weight_decay=1e-4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bddfbc-ccdc-4dd6-9e49-ada4950b5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    global Train_acc\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "        frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "        decay_factor = learning_rate_decay_rate ** frac\n",
    "        current_lr = opt.lr * decay_factor\n",
    "        utils.set_lr(optimizer, current_lr)  # set the decayed rate\n",
    "    else:\n",
    "        current_lr = opt.lr\n",
    "    print('learning_rate: %s' % str(current_lr))\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).sum().item()\n",
    "\n",
    "        utils.progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                           % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    Train_acc = 100.*correct/total\n",
    "    train_acc_list.append(Train_acc)\n",
    "    train_loss_list.append(train_loss / len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c20289d-01d5-4581-a6ea-f74b194271f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing(epoch):\n",
    "    global Testing_acc\n",
    "    global best_Testing_acc\n",
    "    global best_Testing_acc_epoch\n",
    "    net.eval()\n",
    "    Testing_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for batch_idx, (inputs, targets) in enumerate(Testingloader):\n",
    "            bs, ncrops, c, h, w = np.shape(inputs)\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            inputs, targets = inputs, targets\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            outputs_avg = outputs.view(bs, ncrops, -1).mean(1)  # avg over crops\n",
    "            loss = criterion(outputs_avg, targets)\n",
    "            Testing_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs_avg.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "\n",
    "            utils.progress_bar(batch_idx, len(Testingloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                               % (Testing_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    Testing_acc = 100.*correct/total\n",
    "    if Testing_acc > best_Testing_acc:\n",
    "        print('Saving..')\n",
    "        print(\"best_Testing_acc: %0.3f\" % Testing_acc)\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': Testing_acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "        torch.save(state, os.path.join(path, 'Testing_model.t7'))\n",
    "        best_Testing_acc = Testing_acc\n",
    "        best_Testing_acc_epoch = epoch\n",
    "\n",
    "    testing_acc_list.append(Testing_acc)\n",
    "    testing_loss_list.append(Testing_loss / len(Testingloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fe215bb-272f-4073-8648-24bc6f94c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(epoch):\n",
    "    global Validation_acc\n",
    "    global best_Validation_acc\n",
    "    global best_Validation_acc_epoch\n",
    "    net.eval()\n",
    "    Validation_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, (inputs, targets) in enumerate(Validationloader):\n",
    "            bs, ncrops, c, h, w = np.shape(inputs)\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            inputs, targets = inputs, targets\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            outputs_avg = outputs.view(bs, ncrops, -1).mean(1)  # avg over crops\n",
    "            loss = criterion(outputs_avg, targets)\n",
    "            Validation_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs_avg.data, 1)\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "            total += targets.size(0)  # Update the total number of samples\n",
    "\n",
    "            utils.progress_bar(batch_idx, len(Validationloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                               % (Validation_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    Validation_acc = 100. * correct / total\n",
    "    if Validation_acc > best_Validation_acc:\n",
    "        print('Saving..')\n",
    "        print(\"best_Validation_acc: %0.3f\" % Validation_acc)\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': Validation_acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "        torch.save(state, os.path.join(path, 'Validation_model.t7'))\n",
    "        best_Validation_acc = Validation_acc\n",
    "        best_Validation_acc_epoch = epoch\n",
    "\n",
    "    validation_acc_list.append(Validation_acc)\n",
    "    validation_loss_list.append(Validation_loss / len(Validationloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95c1d7a5-986c-4679-87dd-62b2c0a45afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.837 | Acc: 27.821% (7987/28709)     449/449 ===================>..........] | Loss: 1.923 | Acc: 25.190% (4772/18944)     296/449 ======================>.......] | Loss: 1.898 | Acc: 25.758% (5572/21632)     338/449 ========================>.....] | Loss: 1.880 | Acc: 26.326% (6133/23296)     364/449 \n",
      " [=============================>] | Loss: 1.630 | Acc: 34.884% (1252/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 34.884\n",
      " [=============================>] | Loss: 1.630 | Acc: 34.884% (1252/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 34.884\n",
      "\n",
      "Epoch: 1\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.537 | Acc: 40.088% (11509/28709)    449/449 \n",
      " [=============================>] | Loss: 1.390 | Acc: 46.447% (1667/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 46.447\n",
      " [=============================>] | Loss: 1.390 | Acc: 46.447% (1667/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 46.447\n",
      "\n",
      "Epoch: 2\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.424 | Acc: 45.439% (13045/28709)    449/449 \n",
      " [=============================>] | Loss: 1.402 | Acc: 46.921% (1684/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 46.921\n",
      " [=============================>] | Loss: 1.402 | Acc: 46.921% (1684/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 46.921\n",
      "\n",
      "Epoch: 3\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.361 | Acc: 48.093% (13807/28709)    449/449 \n",
      " [=============================>] | Loss: 1.241 | Acc: 52.354% (1879/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 52.354\n",
      " [=============================>] | Loss: 1.241 | Acc: 52.354% (1879/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 52.354\n",
      "\n",
      "Epoch: 4\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.334 | Acc: 48.859% (14027/28709)    449/449 \n",
      " [=============================>] | Loss: 1.245 | Acc: 52.410% (1881/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 52.410\n",
      " [=============================>] | Loss: 1.245 | Acc: 52.410% (1881/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 52.410\n",
      "\n",
      "Epoch: 5\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.305 | Acc: 50.030% (14363/28709)    449/449 \n",
      " [=============================>] | Loss: 1.226 | Acc: 52.522% (1885/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 52.522\n",
      " [=============================>] | Loss: 1.226 | Acc: 52.522% (1885/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 52.522\n",
      "\n",
      "Epoch: 6\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.279 | Acc: 51.231% (14708/28709)    449/449 \n",
      " [=============================>] | Loss: 1.210 | Acc: 53.720% (1928/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 53.720\n",
      " [=============================>] | Loss: 1.210 | Acc: 53.720% (1928/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 53.720\n",
      "\n",
      "Epoch: 7\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.261 | Acc: 52.367% (15034/28709)    449/449 \n",
      " [=============================>] | Loss: 1.168 | Acc: 54.918% (1971/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 54.918\n",
      " [=============================>] | Loss: 1.168 | Acc: 54.918% (1971/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 54.918\n",
      "\n",
      "Epoch: 8\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.248 | Acc: 52.708% (15132/28709)    449/449 \n",
      " [=============================>] | Loss: 1.176 | Acc: 55.531% (1993/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 55.531\n",
      " [=============================>] | Loss: 1.176 | Acc: 55.531% (1993/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 55.531\n",
      "\n",
      "Epoch: 9\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.234 | Acc: 53.377% (15324/28709)    449/449 \n",
      " [=============================>] | Loss: 1.134 | Acc: 55.949% (2008/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 55.949\n",
      " [=============================>] | Loss: 1.134 | Acc: 55.949% (2008/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 55.949\n",
      "\n",
      "Epoch: 10\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.219 | Acc: 53.945% (15487/28709)    449/449 ===============>..............] | Loss: 1.226 | Acc: 53.557% (8055/15040)     235/449 \n",
      " [=============================>] | Loss: 1.162 | Acc: 56.088% (2013/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 56.088\n",
      " [=============================>] | Loss: 1.162 | Acc: 56.088% (2013/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 56.088\n",
      "\n",
      "Epoch: 11\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.206 | Acc: 54.363% (15607/28709)    449/449 376/449 ==========================>...] | Loss: 1.203 | Acc: 54.502% (13778/25280)    395/449 \n",
      " [=============================>] | Loss: 1.107 | Acc: 58.846% (2112/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 58.846\n",
      " [=============================>] | Loss: 1.107 | Acc: 58.846% (2112/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 58.846\n",
      "\n",
      "Epoch: 12\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.195 | Acc: 54.763% (15722/28709)    449/449 ====>........................] | Loss: 1.197 | Acc: 55.329% (2762/4992)      78/449 ========>.....................] | Loss: 1.181 | Acc: 55.369% (4713/8512)      133/449 =======================>......] | Loss: 1.195 | Acc: 54.635% (12553/22976)    359/449 ========================>.....] | Loss: 1.193 | Acc: 54.734% (12821/23424)    366/449 =============================>] | Loss: 1.195 | Acc: 54.772% (15564/28416)    444/449 \n",
      " [=============================>] | Loss: 1.157 | Acc: 56.004% (2010/3589)      57/57 \n",
      " [=============================>] | Loss: 1.157 | Acc: 56.004% (2010/3589)      57/57 \n",
      "\n",
      "Epoch: 13\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.181 | Acc: 55.686% (15987/28709)    449/449 ............................] | Loss: 1.077 | Acc: 61.328% (157/256)        4/449 =>............................] | Loss: 1.197 | Acc: 55.816% (643/1152)       18/449 ==>...........................] | Loss: 1.168 | Acc: 56.392% (1191/2112)      33/449 =====>........................] | Loss: 1.179 | Acc: 55.981% (3117/5568)      87/44 94/449 ===========>..................] | Loss: 1.182 | Acc: 55.885% (6116/10944)     171/449 341/449 ==========================>...] | Loss: 1.177 | Acc: 55.741% (14341/25728)    402/449 ==========================>...] | Loss: 1.177 | Acc: 55.739% (14412/25856)    404/449 >] | Loss: 1.179 | Acc: 55.759% (15773/28288)    442/449 \n",
      " [=============================>] | Loss: 1.124 | Acc: 56.896% (2042/3589)      57/57 \n",
      " [=============================>] | Loss: 1.124 | Acc: 56.896% (2042/3589)      57/57 \n",
      "\n",
      "Epoch: 14\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.175 | Acc: 55.728% (15999/28709)    449/449 /449      112/449 ==========>...................] | Loss: 1.182 | Acc: 55.788% (5677/10176)     159/449 ==============>...............] | Loss: 1.179 | Acc: 55.724% (7632/13696)     214/44 388/449 =============================>] | Loss: 1.175 | Acc: 55.723% (15763/28288)    442/449 \n",
      " [=============================>] | Loss: 1.101 | Acc: 58.679% (2106/3589)      57/57 \n",
      " [=============================>] | Loss: 1.101 | Acc: 58.679% (2106/3589)      57/57 \n",
      "\n",
      "Epoch: 15\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.158 | Acc: 56.198% (16134/28709)    449/449 \n",
      " [=============================>] | Loss: 1.084 | Acc: 58.791% (2110/3589)      57/57 \n",
      " [=============================>] | Loss: 1.084 | Acc: 58.791% (2110/3589)      57/57 \n",
      "\n",
      "Epoch: 16\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.148 | Acc: 56.742% (16290/28709)    449/449 >............................] | Loss: 1.153 | Acc: 56.108% (790/1408)       22/449 =>............................] | Loss: 1.152 | Acc: 56.134% (970/1728)       27/449 ====>.........................] | Loss: 1.147 | Acc: 56.668% (2575/4544)      71/449 =======>......................] | Loss: 1.151 | Acc: 56.620% (4131/7296)      114/449 ========>.....................] | Loss: 1.151 | Acc: 56.625% (4530/8000)      125/449 ========>.....................] | Loss: 1.151 | Acc: 56.613% (4674/8256)      129/449 =========>....................] | Loss: 1.153 | Acc: 56.524% (5354/9472)      148/449 ==========>...................] | Loss: 1.154 | Acc: 56.445% (5491/9728)      152/449 ==========>...................] | Loss: 1.156 | Acc: 56.402% (5559/9856)      154/449 =============>................] | Loss: 1.157 | Acc: 56.374% (7252/12864)     201/449 =============>................] | Loss: 1.157 | Acc: 56.400% (7508/13312)     208/449 ================>.............] | Loss: 1.152 | Acc: 56.505% (8860/15680)     245/449 =================>............] | Loss: 1.152 | Acc: 56.554% (9483/16768)     262/449 ==================>...........] | Loss: 1.150 | Acc: 56.655% (10080/17792)    278/449 ==================>...........] | Loss: 1.150 | Acc: 56.681% (10121/17856)    279/449 ==================>...........] | Loss: 1.150 | Acc: 56.653% (10261/18112)    283/449 ===================>..........] | Loss: 1.149 | Acc: 56.704% (10742/18944)    296/449 ====================>.........] | Loss: 1.150 | Acc: 56.667% (11424/20160)    315/449 =====================>........] | Loss: 1.149 | Acc: 56.635% (11635/20544)    321/449 ======================>.......] | Loss: 1.149 | Acc: 56.584% (12023/21248)    332/449 =======================>......] | Loss: 1.151 | Acc: 56.544% (12702/22464)    351/449 ========================>.....] | Loss: 1.150 | Acc: 56.597% (13547/23936)    374/449 =========================>....] | Loss: 1.150 | Acc: 56.578% (13615/24064)    376/449 \n",
      " [=============================>] | Loss: 1.069 | Acc: 59.738% (2144/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 59.738\n",
      " [=============================>] | Loss: 1.069 | Acc: 59.738% (2144/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 59.738\n",
      "\n",
      "Epoch: 17\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.146 | Acc: 57.038% (16375/28709)    449/449 ............................] | Loss: 1.195 | Acc: 54.375% (348/640)        10/449 =====>........................] | Loss: 1.147 | Acc: 56.571% (2824/4992)      78/449 =====>........................] | Loss: 1.145 | Acc: 56.790% (2944/5184)      81/449 =====>........................] | Loss: 1.138 | Acc: 57.238% (3187/5568)      87/449 ======>.......................] | Loss: 1.136 | Acc: 57.259% (3518/6144)      96/449 =======>......................] | Loss: 1.140 | Acc: 56.940% (4373/7680)      120/449 ===========>..................] | Loss: 1.138 | Acc: 57.167% (6549/11456)     179/449 =============>................] | Loss: 1.142 | Acc: 57.012% (7480/13120)     205/449 ================>.............] | Loss: 1.143 | Acc: 57.050% (9128/16000)     250/449 ==================>...........] | Loss: 1.142 | Acc: 57.048% (10077/17664)    276/449 =======================>......] | Loss: 1.142 | Acc: 56.991% (12766/22400)    350/449 ========================>.....] | Loss: 1.143 | Acc: 56.941% (13265/23296)    364/449 =========================>....] | Loss: 1.144 | Acc: 57.005% (13973/24512)    383/449 ===========================>..] | Loss: 1.145 | Acc: 56.919% (14972/26304)    411/449 ============================>.] | Loss: 1.145 | Acc: 56.948% (15344/26944)    421/449 =============================>] | Loss: 1.145 | Acc: 57.038% (16354/28672)    448/449 \n",
      " [=============================>] | Loss: 1.065 | Acc: 59.320% (2129/3589)      57/57 \n",
      " [============================>.] | Loss: 1.066 | Acc: 59.319% (2126/3584)      56/57 =============================>] | Loss: 1.065 | Acc: 59.320% (2129/3589)      57/57 \n",
      "\n",
      "Epoch: 18\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.135 | Acc: 56.895% (16334/28709)    449/449 ==>..........................] | Loss: 1.123 | Acc: 56.975% (2042/3584)      56/449 =====>........................] | Loss: 1.131 | Acc: 56.927% (3279/5760)      90/449 ========>.....................] | Loss: 1.133 | Acc: 57.202% (4869/8512)      133/449 ============>.................] | Loss: 1.134 | Acc: 57.177% (6660/11648)     182/449 =============>................] | Loss: 1.131 | Acc: 57.169% (7281/12736)     199/449 =============>................] | Loss: 1.132 | Acc: 57.180% (7319/12800)     200/449 =============>................] | Loss: 1.133 | Acc: 57.158% (7682/13440)     210/449 ==================>...........] | Loss: 1.129 | Acc: 57.147% (10387/18176)    284/449 \n",
      " [=============================>] | Loss: 1.068 | Acc: 59.181% (2124/3589)      57/57 \n",
      " [=============================>] | Loss: 1.068 | Acc: 59.181% (2124/3589)      57/57 \n",
      "\n",
      "Epoch: 19\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.127 | Acc: 57.289% (16447/28709)    449/449 ====================>.........] | Loss: 1.121 | Acc: 57.413% (11501/20032)    313/449 \n",
      " [=============================>] | Loss: 1.058 | Acc: 58.986% (2117/3589)      57/57 \n",
      " [=============================>] | Loss: 1.058 | Acc: 58.986% (2117/3589)      57/57 \n",
      "\n",
      "Epoch: 20\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.123 | Acc: 57.877% (16616/28709)    449/449 =========>....................] | Loss: 1.112 | Acc: 57.990% (5567/9600)      150/449 ===========>..................] | Loss: 1.112 | Acc: 58.138% (6251/10752)     168/449 ===========>..................] | Loss: 1.112 | Acc: 58.109% (6657/11456)     179/449 \n",
      " [=============================>] | Loss: 1.052 | Acc: 59.459% (2134/3589)      57/57 \n",
      " [=============================>] | Loss: 1.052 | Acc: 59.459% (2134/3589)      57/57 \n",
      "\n",
      "Epoch: 21\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.123 | Acc: 57.609% (16539/28709)    449/449 =============>................] | Loss: 1.123 | Acc: 57.597% (7741/13440)     210/449 ==============>...............] | Loss: 1.122 | Acc: 57.599% (7815/13568)     212/449 =================>............] | Loss: 1.122 | Acc: 57.627% (9626/16704)     261/449 =================>............] | Loss: 1.122 | Acc: 57.613% (9808/17024)     266/449 ===================>..........] | Loss: 1.124 | Acc: 57.468% (10997/19136)    299/449 ============================>.] | Loss: 1.124 | Acc: 57.612% (15781/27392)    428/449 =============================>] | Loss: 1.124 | Acc: 57.619% (16410/28480)    445/449 \n",
      " [=============================>] | Loss: 1.040 | Acc: 59.794% (2146/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 59.794\n",
      " [=============================>] | Loss: 1.040 | Acc: 59.794% (2146/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 59.794\n",
      "\n",
      "Epoch: 22\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.106 | Acc: 58.194% (16707/28709)    449/449 \n",
      " [=============================>] | Loss: 1.027 | Acc: 60.379% (2167/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 60.379\n",
      " [=============================>] | Loss: 1.027 | Acc: 60.379% (2167/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 60.379\n",
      "\n",
      "Epoch: 23\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.109 | Acc: 58.560% (16812/28709)    449/449 >............................] | Loss: 1.078 | Acc: 59.588% (839/1408)       22/449 =========>....................] | Loss: 1.102 | Acc: 59.288% (5464/9216)      144/449 =========>....................] | Loss: 1.102 | Acc: 59.259% (5613/9472)      148/449 =========>....................] | Loss: 1.102 | Acc: 59.260% (5689/9600)      150/449 ===========>..................] | Loss: 1.099 | Acc: 59.209% (6404/10816)     169/449 ============>.................] | Loss: 1.100 | Acc: 59.149% (7344/12416)     194/449 ========================>.....] | Loss: 1.105 | Acc: 58.600% (13914/23744)    371/449 \n",
      " [=============================>] | Loss: 1.044 | Acc: 60.518% (2172/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 60.518\n",
      " [=============================>] | Loss: 1.044 | Acc: 60.518% (2172/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 60.518\n",
      "\n",
      "Epoch: 24\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.098 | Acc: 58.372% (16758/28709)    449/449 \n",
      " [=============================>] | Loss: 1.039 | Acc: 60.351% (2166/3589)      57/57 \n",
      " [=============================>] | Loss: 1.039 | Acc: 60.351% (2166/3589)      57/57 \n",
      "\n",
      "Epoch: 25\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.090 | Acc: 58.658% (16840/28709)    449/449 ===>.........................] | Loss: 1.076 | Acc: 59.225% (2767/4672)      73/449 ========>.....................] | Loss: 1.081 | Acc: 59.040% (4761/8064)      126/449 \n",
      " [=============================>] | Loss: 1.028 | Acc: 61.716% (2215/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 61.716\n",
      " [=============================>] | Loss: 1.028 | Acc: 61.716% (2215/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 61.716\n",
      "\n",
      "Epoch: 26\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.090 | Acc: 58.786% (16877/28709)    449/449 ===>.........................] | Loss: 1.099 | Acc: 58.507% (2696/4608)      72/449 ====>.........................] | Loss: 1.099 | Acc: 58.497% (2733/4672)      73/449 ==========>...................] | Loss: 1.083 | Acc: 59.230% (6141/10368)     162/449 ==================>...........] | Loss: 1.085 | Acc: 58.994% (10232/17344)    271/449 ==================>...........] | Loss: 1.087 | Acc: 58.973% (10417/17664)    276/449 ==================>...........] | Loss: 1.087 | Acc: 58.959% (10490/17792)    278/449 ===================>..........] | Loss: 1.088 | Acc: 58.971% (10794/18304)    286/449 =======================>......] | Loss: 1.086 | Acc: 59.038% (13300/22528)    352/449 =======================>......] | Loss: 1.086 | Acc: 59.017% (13371/22656)    354/449 ========================>.....] | Loss: 1.087 | Acc: 58.990% (13780/23360)    365/449 ========================>.....] | Loss: 1.087 | Acc: 58.965% (13812/23424)    366/449 =========================>....] | Loss: 1.090 | Acc: 58.892% (14398/24448)    382/449 ===========================>..] | Loss: 1.090 | Acc: 58.881% (15601/26496)    414/449 \n",
      " [=============================>] | Loss: 1.058 | Acc: 59.125% (2122/3589)      57/57 \n",
      " [=============================>] | Loss: 1.058 | Acc: 59.125% (2122/3589)      57/57 \n",
      "\n",
      "Epoch: 27\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.084 | Acc: 59.128% (16975/28709)    449/449 ===>.........................] | Loss: 1.089 | Acc: 59.076% (2571/4352)      68/449 =====>........................] | Loss: 1.081 | Acc: 59.513% (3009/5056)      79/449 =======>......................] | Loss: 1.076 | Acc: 59.674% (4392/7360)      115/449 121/449 ===============>..............] | Loss: 1.085 | Acc: 59.164% (8974/15168)     237/449 =================>............] | Loss: 1.077 | Acc: 59.451% (10121/17024)    266/449 ==================>...........] | Loss: 1.078 | Acc: 59.477% (10544/17728)    277/449 =====================>........] | Loss: 1.083 | Acc: 59.220% (12204/20608)    322/449 =======================>......] | Loss: 1.082 | Acc: 59.295% (13358/22528)    352/449 =========================>....] | Loss: 1.080 | Acc: 59.404% (14485/24384)    381/449 \n",
      " [=============================>] | Loss: 1.019 | Acc: 60.379% (2167/3589)      57/57 \n",
      " [=============================>] | Loss: 1.019 | Acc: 60.379% (2167/3589)      57/57 \n",
      "\n",
      "Epoch: 28\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.080 | Acc: 59.347% (17038/28709)    449/449 8/449 ==========>...................] | Loss: 1.081 | Acc: 59.210% (6101/10304)     161/449 213/44 291/449 \n",
      " [=============================>] | Loss: 1.030 | Acc: 60.630% (2176/3589)      57/57 \n",
      " [=============================>] | Loss: 1.030 | Acc: 60.630% (2176/3589)      57/57 \n",
      "\n",
      "Epoch: 29\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.080 | Acc: 59.413% (17057/28709)    449/449 =>...........................] | Loss: 1.063 | Acc: 58.972% (1170/1984)      31/449 ====>.........................] | Loss: 1.066 | Acc: 59.042% (2834/4800)      75/449 ================>.............] | Loss: 1.077 | Acc: 59.631% (9541/16000)     250/449 ====================>.........] | Loss: 1.073 | Acc: 59.742% (12044/20160)    315/449 =====================>........] | Loss: 1.072 | Acc: 59.793% (12169/20352)    318/449 ======================>.......] | Loss: 1.073 | Acc: 59.699% (12723/21312)    333/449 =======================>......] | Loss: 1.073 | Acc: 59.676% (13482/22592)    353/449 ========================>.....] | Loss: 1.074 | Acc: 59.693% (14059/23552)    368/449 \n",
      " [=============================>] | Loss: 1.035 | Acc: 60.936% (2187/3589)      57/57 \n",
      " [=============================>] | Loss: 1.035 | Acc: 60.936% (2187/3589)      57/57 \n",
      "\n",
      "Epoch: 30\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.070 | Acc: 59.358% (17041/28709)    449/449 .............................] | Loss: 1.101 | Acc: 56.362% (505/896)        14/449 \n",
      " [=============================>] | Loss: 1.007 | Acc: 62.106% (2229/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 62.106\n",
      " [=============================>] | Loss: 1.007 | Acc: 62.106% (2229/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 62.106\n",
      "\n",
      "Epoch: 31\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.063 | Acc: 59.985% (17221/28709)    449/449 =========>....................] | Loss: 1.053 | Acc: 60.262% (5438/9024)      141/449 =============>................] | Loss: 1.057 | Acc: 60.248% (7866/13056)     204/449 =============>................] | Loss: 1.058 | Acc: 60.212% (8054/13376)     209/449 ================>.............] | Loss: 1.062 | Acc: 60.084% (9575/15936)     249/449  312/449 ========================>.....] | Loss: 1.063 | Acc: 60.025% (14406/24000)    375/449 \n",
      " [=============================>] | Loss: 1.027 | Acc: 61.298% (2200/3589)      57/57 \n",
      " [=============================>] | Loss: 1.027 | Acc: 61.298% (2200/3589)      57/57 \n",
      "\n",
      "Epoch: 32\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.061 | Acc: 59.915% (17201/28709)    449/449 ===============>..............] | Loss: 1.059 | Acc: 59.857% (9194/15360)     240/449 ====================>.........] | Loss: 1.051 | Acc: 60.063% (11955/19904)    311/449 ======================>.......] | Loss: 1.054 | Acc: 59.940% (12736/21248)    332/449 ========================>.....] | Loss: 1.053 | Acc: 60.102% (14386/23936)    374/449 =========================>....] | Loss: 1.056 | Acc: 59.991% (14897/24832)    388/449 \n",
      " [=============================>] | Loss: 1.005 | Acc: 62.580% (2246/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 62.580\n",
      " [=============================>] | Loss: 1.005 | Acc: 62.580% (2246/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 62.580\n",
      "\n",
      "Epoch: 33\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.061 | Acc: 60.037% (17236/28709)    449/449 .............................] | Loss: 1.132 | Acc: 57.422% (441/768)        12/449 ====>.........................] | Loss: 1.094 | Acc: 58.861% (2750/4672)      73/449 =====>........................] | Loss: 1.080 | Acc: 59.601% (3166/5312)      83/449 ======>.......................] | Loss: 1.076 | Acc: 59.562% (3812/6400)      100/449 =======>......................] | Loss: 1.069 | Acc: 59.851% (4405/7360)      115/449 ================>.............] | Loss: 1.062 | Acc: 59.995% (9292/15488)     242/449 ==================>...........] | Loss: 1.061 | Acc: 60.095% (10769/17920)    280/449 ==================>...........] | Loss: 1.060 | Acc: 60.181% (10900/18112)    283/449 ========================>.....] | Loss: 1.062 | Acc: 59.966% (14008/23360)    365/44 375/449 ==========================>...] | Loss: 1.062 | Acc: 59.923% (15187/25344)    396/449 \n",
      " [=============================>] | Loss: 1.014 | Acc: 61.577% (2210/3589)      57/57 \n",
      " [=============================>] | Loss: 1.014 | Acc: 61.577% (2210/3589)      57/57 \n",
      "\n",
      "Epoch: 34\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.061 | Acc: 60.072% (17246/28709)    449/449 ............................] | Loss: 1.044 | Acc: 60.156% (308/512)        8/449 ======>.......................] | Loss: 1.083 | Acc: 58.641% (3753/6400)      100/449  | Loss: 1.084 | Acc: 58.478% (4566/7808)      122/449 =========>....................] | Loss: 1.077 | Acc: 58.847% (5235/8896)      139/449 =========>....................] | Loss: 1.076 | Acc: 58.898% (5315/9024)      141/449 235/449 \n",
      " [=============================>] | Loss: 1.020 | Acc: 61.549% (2209/3589)      57/57 \n",
      " [=============================>] | Loss: 1.020 | Acc: 61.549% (2209/3589)      57/57 \n",
      "\n",
      "Epoch: 35\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.051 | Acc: 60.152% (17269/28709)    449/449 ============================>.] | Loss: 1.051 | Acc: 60.125% (16200/26944)    421/449 \n",
      " [=============================>] | Loss: 1.008 | Acc: 61.772% (2217/3589)      57/57 \n",
      " [=============================>] | Loss: 1.008 | Acc: 61.772% (2217/3589)      57/57 \n",
      "\n",
      "Epoch: 36\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.050 | Acc: 60.437% (17351/28709)    449/449 =>...........................] | Loss: 1.063 | Acc: 59.848% (1264/2112)      33/449 ===>..........................] | Loss: 1.064 | Acc: 59.528% (1943/3264)      51/449 \n",
      " [=============================>] | Loss: 1.008 | Acc: 62.441% (2241/3589)      57/57 \n",
      " [=============================>] | Loss: 1.008 | Acc: 62.441% (2241/3589)      57/57 \n",
      "\n",
      "Epoch: 37\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.057 | Acc: 60.239% (17294/28709)    449/449 \n",
      " [=============================>] | Loss: 1.038 | Acc: 60.240% (2162/3589)      57/57 \n",
      " [=============================>] | Loss: 1.038 | Acc: 60.240% (2162/3589)      57/57 \n",
      "\n",
      "Epoch: 38\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.039 | Acc: 60.908% (17486/28709)    449/449 \n",
      " [=============================>] | Loss: 1.009 | Acc: 61.271% (2199/3589)      57/57 \n",
      " [=============================>] | Loss: 1.009 | Acc: 61.271% (2199/3589)      57/57 \n",
      "\n",
      "Epoch: 39\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.037 | Acc: 60.678% (17420/28709)    449/449 \n",
      " [=============================>] | Loss: 1.040 | Acc: 60.351% (2166/3589)      57/57 \n",
      " [=============================>] | Loss: 1.040 | Acc: 60.351% (2166/3589)      57/57 \n",
      "\n",
      "Epoch: 40\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.038 | Acc: 61.106% (17543/28709)    449/449 \n",
      " [=============================>] | Loss: 0.989 | Acc: 62.692% (2250/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 62.692\n",
      " [=============================>] | Loss: 0.989 | Acc: 62.692% (2250/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 62.692\n",
      "\n",
      "Epoch: 41\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.033 | Acc: 60.998% (17512/28709)    449/449 \n",
      " [=============================>] | Loss: 1.008 | Acc: 61.772% (2217/3589)      57/57 \n",
      " [=============================>] | Loss: 1.008 | Acc: 61.772% (2217/3589)      57/57 \n",
      "\n",
      "Epoch: 42\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.029 | Acc: 61.350% (17613/28709)    449/449 \n",
      " [=============================>] | Loss: 0.995 | Acc: 62.636% (2248/3589)      57/57 \n",
      " [=============================>] | Loss: 0.995 | Acc: 62.636% (2248/3589)      57/57 \n",
      "\n",
      "Epoch: 43\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.032 | Acc: 61.284% (17594/28709)    449/449 \n",
      " [=============================>] | Loss: 0.994 | Acc: 61.466% (2206/3589)      57/57 \n",
      " [=============================>] | Loss: 0.994 | Acc: 61.466% (2206/3589)      57/57 \n",
      "\n",
      "Epoch: 44\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.021 | Acc: 61.395% (17626/28709)    449/449 \n",
      " [=============================>] | Loss: 1.016 | Acc: 62.497% (2243/3589)      57/57 \n",
      " [=============================>] | Loss: 1.016 | Acc: 62.497% (2243/3589)      57/57 \n",
      "\n",
      "Epoch: 45\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.029 | Acc: 61.211% (17573/28709)    449/449 \n",
      " [============================>.] | Loss: 0.980 | Acc: 63.086% (2261/3584)      56/57 =============================>] | Loss: 0.978 | Acc: 63.082% (2264/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 63.082\n",
      " [=============================>] | Loss: 0.978 | Acc: 63.082% (2264/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 63.082\n",
      "\n",
      "Epoch: 46\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.020 | Acc: 61.733% (17723/28709)    449/449 \n",
      " [=============================>] | Loss: 1.004 | Acc: 62.106% (2229/3589)      57/57 \n",
      " [=============================>] | Loss: 1.004 | Acc: 62.106% (2229/3589)      57/57 \n",
      "\n",
      "Epoch: 47\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.018 | Acc: 61.733% (17723/28709)    449/449 \n",
      " [=============================>] | Loss: 0.992 | Acc: 61.605% (2211/3589)      57/57 \n",
      " [=============================>] | Loss: 0.992 | Acc: 61.605% (2211/3589)      57/57 \n",
      "\n",
      "Epoch: 48\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.011 | Acc: 61.615% (17689/28709)    449/449 \n",
      " [=============================>] | Loss: 0.986 | Acc: 62.246% (2234/3589)      57/57 \n",
      " [=============================>] | Loss: 0.986 | Acc: 62.246% (2234/3589)      57/57 \n",
      "\n",
      "Epoch: 49\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.021 | Acc: 61.604% (17686/28709)    449/449 \n",
      " [=============================>] | Loss: 0.989 | Acc: 62.218% (2233/3589)      57/57 \n",
      " [============================>.] | Loss: 0.988 | Acc: 62.221% (2230/3584)      56/57 =============================>] | Loss: 0.989 | Acc: 62.218% (2233/3589)      57/57 \n",
      "\n",
      "Epoch: 50\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.011 | Acc: 62.102% (17829/28709)    449/449 \n",
      " [=============================>] | Loss: 0.981 | Acc: 62.329% (2237/3589)      57/57 \n",
      " [=============================>] | Loss: 0.981 | Acc: 62.329% (2237/3589)      57/57 \n",
      "\n",
      "Epoch: 51\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.016 | Acc: 61.712% (17717/28709)    449/449 \n",
      " [=============================>] | Loss: 0.977 | Acc: 63.165% (2267/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 63.165\n",
      " [=============================>] | Loss: 0.977 | Acc: 63.165% (2267/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 63.165\n",
      "\n",
      "Epoch: 52\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.008 | Acc: 62.193% (17855/28709)    449/449 \n",
      " [=============================>] | Loss: 0.976 | Acc: 63.583% (2282/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 63.583\n",
      " [=============================>] | Loss: 0.976 | Acc: 63.583% (2282/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 63.583\n",
      "\n",
      "Epoch: 53\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.004 | Acc: 62.419% (17920/28709)    449/449 \n",
      " [=============================>] | Loss: 0.986 | Acc: 63.110% (2265/3589)      57/57 \n",
      " [=============================>] | Loss: 0.986 | Acc: 63.110% (2265/3589)      57/57 \n",
      "\n",
      "Epoch: 54\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.002 | Acc: 62.235% (17867/28709)    449/449 \n",
      " [=============================>] | Loss: 0.996 | Acc: 62.664% (2249/3589)      57/57 \n",
      " [=============================>] | Loss: 0.996 | Acc: 62.664% (2249/3589)      57/57 \n",
      "\n",
      "Epoch: 55\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 1.004 | Acc: 62.158% (17845/28709)    449/449 \n",
      " [=============================>] | Loss: 0.993 | Acc: 62.218% (2233/3589)      57/57 \n",
      " [=============================>] | Loss: 0.993 | Acc: 62.218% (2233/3589)      57/57 \n",
      "\n",
      "Epoch: 56\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 0.997 | Acc: 62.350% (17900/28709)    449/449 \n",
      " [=============================>] | Loss: 0.981 | Acc: 63.890% (2293/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 63.890\n",
      " [=============================>] | Loss: 0.981 | Acc: 63.890% (2293/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 63.890\n",
      "\n",
      "Epoch: 57\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 0.992 | Acc: 63.039% (18098/28709)    449/449 \n",
      " [=============================>] | Loss: 0.982 | Acc: 63.137% (2266/3589)      57/57 \n",
      " [=============================>] | Loss: 0.982 | Acc: 63.137% (2266/3589)      57/57 \n",
      "\n",
      "Epoch: 58\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 0.996 | Acc: 62.712% (18004/28709)    449/449 \n",
      " [=============================>] | Loss: 0.966 | Acc: 63.444% (2277/3589)      57/57 \n",
      " [=============================>] | Loss: 0.966 | Acc: 63.444% (2277/3589)      57/57 \n",
      "\n",
      "Epoch: 59\n",
      "learning_rate: 0.001\n",
      " [=============================>] | Loss: 0.991 | Acc: 63.161% (18133/28709)    449/449 \n",
      " [=============================>] | Loss: 0.985 | Acc: 62.998% (2261/3589)      57/57 \n",
      " [=============================>] | Loss: 0.985 | Acc: 62.998% (2261/3589)      57/57 \n",
      "\n",
      "Epoch: 60\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.950 | Acc: 64.551% (18532/28709)    449/449 \n",
      " [=============================>] | Loss: 0.945 | Acc: 64.809% (2326/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 64.809\n",
      " [=============================>] | Loss: 0.945 | Acc: 64.809% (2326/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 64.809\n",
      "\n",
      "Epoch: 61\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.936 | Acc: 64.906% (18634/28709)    449/449 \n",
      " [=============================>] | Loss: 0.968 | Acc: 63.305% (2272/3589)      57/57 \n",
      " [=============================>] | Loss: 0.968 | Acc: 63.305% (2272/3589)      57/57 \n",
      "\n",
      "Epoch: 62\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.924 | Acc: 65.304% (18748/28709)    449/449 \n",
      " [=============================>] | Loss: 0.930 | Acc: 66.147% (2374/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 66.147\n",
      " [=============================>] | Loss: 0.930 | Acc: 66.147% (2374/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 66.147\n",
      "\n",
      "Epoch: 63\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.916 | Acc: 65.718% (18867/28709)    449/449 \n",
      " [=============================>] | Loss: 0.940 | Acc: 65.004% (2333/3589)      57/57 \n",
      " [=============================>] | Loss: 0.940 | Acc: 65.004% (2333/3589)      57/57 \n",
      "\n",
      "Epoch: 64\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.918 | Acc: 65.568% (18824/28709)    449/449 \n",
      " [=============================>] | Loss: 0.937 | Acc: 64.921% (2330/3589)      57/57 \n",
      " [=============================>] | Loss: 0.937 | Acc: 64.921% (2330/3589)      57/57 \n",
      "\n",
      "Epoch: 65\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.910 | Acc: 65.951% (18934/28709)    449/449 \n",
      " [=============================>] | Loss: 0.934 | Acc: 64.670% (2321/3589)      57/57 \n",
      " [=============================>] | Loss: 0.934 | Acc: 64.670% (2321/3589)      57/57 \n",
      "\n",
      "Epoch: 66\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.905 | Acc: 66.018% (18953/28709)    449/449 \n",
      " [=============================>] | Loss: 0.929 | Acc: 65.339% (2345/3589)      57/57 \n",
      " [=============================>] | Loss: 0.929 | Acc: 65.339% (2345/3589)      57/57 \n",
      "\n",
      "Epoch: 67\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.907 | Acc: 65.701% (18862/28709)    449/449 \n",
      " [=============================>] | Loss: 0.940 | Acc: 64.503% (2315/3589)      57/57 \n",
      " [=============================>] | Loss: 0.940 | Acc: 64.503% (2315/3589)      57/57 \n",
      "\n",
      "Epoch: 68\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.900 | Acc: 66.227% (19013/28709)    449/449 \n",
      " [=============================>] | Loss: 0.920 | Acc: 65.645% (2356/3589)      57/57 \n",
      " [=============================>] | Loss: 0.920 | Acc: 65.645% (2356/3589)      57/57 \n",
      "\n",
      "Epoch: 69\n",
      "learning_rate: 0.0005\n",
      " [=============================>] | Loss: 0.895 | Acc: 66.279% (19028/28709)    449/449 \n",
      " [=============================>] | Loss: 0.940 | Acc: 64.781% (2325/3589)      57/57 \n",
      " [=============================>] | Loss: 0.940 | Acc: 64.781% (2325/3589)      57/57 \n",
      "\n",
      "Epoch: 70\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.866 | Acc: 67.376% (19343/28709)    449/449 \n",
      " [=============================>] | Loss: 0.927 | Acc: 66.091% (2372/3589)      57/57 \n",
      " [=============================>] | Loss: 0.927 | Acc: 66.091% (2372/3589)      57/57 \n",
      "\n",
      "Epoch: 71\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.861 | Acc: 67.536% (19389/28709)    449/449 \n",
      " [=============================>] | Loss: 0.922 | Acc: 65.812% (2362/3589)      57/57 \n",
      " [=============================>] | Loss: 0.922 | Acc: 65.812% (2362/3589)      57/57 \n",
      "\n",
      "Epoch: 72\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.858 | Acc: 67.710% (19439/28709)    449/449 \n",
      " [=============================>] | Loss: 0.909 | Acc: 66.119% (2373/3589)      57/57 \n",
      " [=============================>] | Loss: 0.909 | Acc: 66.119% (2373/3589)      57/57 \n",
      "\n",
      "Epoch: 73\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.852 | Acc: 68.181% (19574/28709)    449/449 \n",
      " [=============================>] | Loss: 0.911 | Acc: 66.258% (2378/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 66.258\n",
      " [=============================>] | Loss: 0.911 | Acc: 66.258% (2378/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 66.258\n",
      "\n",
      "Epoch: 74\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.850 | Acc: 68.003% (19523/28709)    449/449 \n",
      " [=============================>] | Loss: 0.912 | Acc: 66.174% (2375/3589)      57/57 \n",
      " [=============================>] | Loss: 0.912 | Acc: 66.174% (2375/3589)      57/57 \n",
      "\n",
      "Epoch: 75\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.848 | Acc: 68.432% (19646/28709)    449/449 \n",
      " [=============================>] | Loss: 0.908 | Acc: 66.899% (2401/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 66.899\n",
      " [=============================>] | Loss: 0.908 | Acc: 66.899% (2401/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 66.899\n",
      "\n",
      "Epoch: 76\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.846 | Acc: 68.223% (19586/28709)    449/449 \n",
      " [=============================>] | Loss: 0.909 | Acc: 66.481% (2386/3589)      57/57 \n",
      " [=============================>] | Loss: 0.909 | Acc: 66.481% (2386/3589)      57/57 \n",
      "\n",
      "Epoch: 77\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.844 | Acc: 68.268% (19599/28709)    449/449 \n",
      " [=============================>] | Loss: 0.911 | Acc: 66.565% (2389/3589)      57/57 \n",
      " [=============================>] | Loss: 0.911 | Acc: 66.565% (2389/3589)      57/57 \n",
      "\n",
      "Epoch: 78\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.839 | Acc: 68.588% (19691/28709)    449/449 \n",
      " [=============================>] | Loss: 0.912 | Acc: 66.369% (2382/3589)      57/57 \n",
      " [=============================>] | Loss: 0.912 | Acc: 66.369% (2382/3589)      57/57 \n",
      "\n",
      "Epoch: 79\n",
      "learning_rate: 0.00025\n",
      " [=============================>] | Loss: 0.838 | Acc: 68.477% (19659/28709)    449/449 \n",
      " [=============================>] | Loss: 0.904 | Acc: 66.509% (2387/3589)      57/57 \n",
      " [=============================>] | Loss: 0.904 | Acc: 66.509% (2387/3589)      57/57 \n",
      "\n",
      "Epoch: 80\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.818 | Acc: 69.299% (19895/28709)    449/449 \n",
      " [=============================>] | Loss: 0.900 | Acc: 66.815% (2398/3589)      57/57 \n",
      " [=============================>] | Loss: 0.900 | Acc: 66.815% (2398/3589)      57/57 \n",
      "\n",
      "Epoch: 81\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.813 | Acc: 69.386% (19920/28709)    449/449 \n",
      " [=============================>] | Loss: 0.907 | Acc: 66.537% (2388/3589)      57/57 \n",
      " [=============================>] | Loss: 0.907 | Acc: 66.537% (2388/3589)      57/57 \n",
      "\n",
      "Epoch: 82\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.807 | Acc: 69.800% (20039/28709)    449/449 \n",
      " [=============================>] | Loss: 0.898 | Acc: 67.233% (2413/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 67.233\n",
      " [=============================>] | Loss: 0.898 | Acc: 67.233% (2413/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 67.233\n",
      "\n",
      "Epoch: 83\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.812 | Acc: 69.584% (19977/28709)    449/449 \n",
      " [=============================>] | Loss: 0.915 | Acc: 66.258% (2378/3589)      57/57 \n",
      " [=============================>] | Loss: 0.915 | Acc: 66.258% (2378/3589)      57/57 \n",
      "\n",
      "Epoch: 84\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.809 | Acc: 69.595% (19980/28709)    449/449 \n",
      " [=============================>] | Loss: 0.904 | Acc: 66.119% (2373/3589)      57/57 \n",
      " [=============================>] | Loss: 0.904 | Acc: 66.119% (2373/3589)      57/57 \n",
      "\n",
      "Epoch: 85\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.807 | Acc: 69.874% (20060/28709)    449/449 \n",
      " [=============================>] | Loss: 0.906 | Acc: 66.704% (2394/3589)      57/57 \n",
      " [=============================>] | Loss: 0.906 | Acc: 66.704% (2394/3589)      57/57 \n",
      "\n",
      "Epoch: 86\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.809 | Acc: 69.908% (20070/28709)    449/449 \n",
      " [=============================>] | Loss: 0.899 | Acc: 67.177% (2411/3589)      57/57 \n",
      " [=============================>] | Loss: 0.899 | Acc: 67.177% (2411/3589)      57/57 \n",
      "\n",
      "Epoch: 87\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.800 | Acc: 70.156% (20141/28709)    449/449 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.676% (2393/3589)      57/57 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.676% (2393/3589)      57/57 \n",
      "\n",
      "Epoch: 88\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.796 | Acc: 70.389% (20208/28709)    449/449 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.927% (2402/3589)      57/57 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.927% (2402/3589)      57/57 \n",
      "\n",
      "Epoch: 89\n",
      "learning_rate: 0.000125\n",
      " [=============================>] | Loss: 0.802 | Acc: 70.044% (20109/28709)    449/449 \n",
      " [=============================>] | Loss: 0.894 | Acc: 66.760% (2396/3589)      57/57 \n",
      " [=============================>] | Loss: 0.894 | Acc: 66.760% (2396/3589)      57/57 \n",
      "\n",
      "Epoch: 90\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.785 | Acc: 70.723% (20304/28709)    449/449 \n",
      " [=============================>] | Loss: 0.898 | Acc: 66.927% (2402/3589)      57/57 \n",
      " [=============================>] | Loss: 0.898 | Acc: 66.927% (2402/3589)      57/57 \n",
      "\n",
      "Epoch: 91\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.784 | Acc: 70.438% (20222/28709)    449/449 \n",
      " [=============================>] | Loss: 0.897 | Acc: 67.317% (2416/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 67.317\n",
      " [=============================>] | Loss: 0.897 | Acc: 67.317% (2416/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 67.317\n",
      "\n",
      "Epoch: 92\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.785 | Acc: 70.501% (20240/28709)    449/449 \n",
      " [=============================>] | Loss: 0.902 | Acc: 67.317% (2416/3589)      57/57 \n",
      " [=============================>] | Loss: 0.902 | Acc: 67.317% (2416/3589)      57/57 \n",
      "\n",
      "Epoch: 93\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.785 | Acc: 70.762% (20315/28709)    449/449 \n",
      " [=============================>] | Loss: 0.893 | Acc: 67.261% (2414/3589)      57/57 \n",
      " [=============================>] | Loss: 0.893 | Acc: 67.261% (2414/3589)      57/57 \n",
      "\n",
      "Epoch: 94\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.784 | Acc: 71.006% (20385/28709)    449/449 \n",
      " [=============================>] | Loss: 0.901 | Acc: 67.066% (2407/3589)      57/57 \n",
      " [=============================>] | Loss: 0.901 | Acc: 67.066% (2407/3589)      57/57 \n",
      "\n",
      "Epoch: 95\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.780 | Acc: 70.671% (20289/28709)    449/449 \n",
      " [=============================>] | Loss: 0.900 | Acc: 67.066% (2407/3589)      57/57 \n",
      " [=============================>] | Loss: 0.900 | Acc: 67.066% (2407/3589)      57/57 \n",
      "\n",
      "Epoch: 96\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.779 | Acc: 71.173% (20433/28709)    449/449 \n",
      " [=============================>] | Loss: 0.898 | Acc: 67.373% (2418/3589)      57/57 \n",
      "Saving..\n",
      "best_Testing_acc: 67.373\n",
      " [=============================>] | Loss: 0.898 | Acc: 67.373% (2418/3589)      57/57 \n",
      "Saving..\n",
      "best_Validation_acc: 67.373\n",
      "\n",
      "Epoch: 97\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.778 | Acc: 71.033% (20393/28709)    449/449 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.592% (2390/3589)      57/57 \n",
      " [=============================>] | Loss: 0.902 | Acc: 66.592% (2390/3589)      57/57 \n",
      "\n",
      "Epoch: 98\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.783 | Acc: 70.703% (20298/28709)    449/449 \n",
      " [=============================>] | Loss: 0.899 | Acc: 67.373% (2418/3589)      57/57 \n",
      " [=============================>] | Loss: 0.899 | Acc: 67.373% (2418/3589)      57/57 \n",
      "\n",
      "Epoch: 99\n",
      "learning_rate: 6.25e-05\n",
      " [=============================>] | Loss: 0.775 | Acc: 71.027% (20391/28709)    449/449 \n",
      " [=============================>] | Loss: 0.905 | Acc: 66.982% (2404/3589)      57/57 \n",
      " [=============================>] | Loss: 0.905 | Acc: 66.982% (2404/3589)      57/57 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, total_epoch):\n",
    "    train(epoch)\n",
    "    Testing(epoch)\n",
    "    Validation(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b15d3821-378a-4987-92d3-365d51e41653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Testing accuracy: 67.373% at epoch 96\n",
      "Best Validation accuracy: 67.373% at epoch 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Testing accuracy: %0.3f%% at epoch %d\" % (best_Testing_acc, best_Testing_acc_epoch))\n",
    "print(\"Best Validation accuracy: %0.3f%% at epoch %d\" % (best_Validation_acc, best_Validation_acc_epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Facial Expression Recognition",
   "language": "python",
   "name": "facialexpressionrecogination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
