{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf046c9-e250-4bae-9396-8d47387925ca",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8509689-1c6e-453f-a663-2988efe8581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import utils\n",
    "from fer import FER2013\n",
    "from torch.autograd import Variable\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797806b0-80e6-4d11-bee1-eaa01131edc5",
   "metadata": {},
   "source": [
    "**Parse Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e89a8a9-2302-4580-bd56-14a416735975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.model = 'ResNet18'\n",
    "        self.dataset = 'FER2013'\n",
    "        self.bs = 128\n",
    "        self.lr = 0.01\n",
    "        self.resume = True\n",
    "\n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09b43f-7474-470c-8a65-f110f68cee64",
   "metadata": {},
   "source": [
    "**Setup and Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dac3b3a-fcce-41a5-9dee-8c44dd9bb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "best_Testing_acc = 0 \n",
    "best_Testing_acc_epoch = 0\n",
    "best_Validation_acc = 0 \n",
    "best_Validation_acc_epoch = 0\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "learning_rate_decay_start = 80  # 50\n",
    "learning_rate_decay_every = 5   # 5\n",
    "learning_rate_decay_rate = 0.9  # 0.9\n",
    "\n",
    "cut_size = 44\n",
    "total_epoch = 100\n",
    "\n",
    "path = os.path.join(opt.dataset + '_' + opt.model)\n",
    "\n",
    "#lists to save metrices\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "testing_acc_list = []\n",
    "testing_loss_list = []\n",
    "validation_acc_list = []\n",
    "validation_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfc0e1-377a-4db9-a50a-eff18d5c3567",
   "metadata": {},
   "source": [
    "**Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dc39bf-7d43-4152-affe-6f855d63d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(44),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "trainset = FER2013(split='Training', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.bs, shuffle=True, num_workers=0)\n",
    "Testingset = FER2013(split='Testing', transform=transform_test)\n",
    "Testingloader = torch.utils.data.DataLoader(Testingset, batch_size=opt.bs, shuffle=False, num_workers=0)\n",
    "Validationset = FER2013(split='Validation', transform=transform_test)\n",
    "Validationloader = torch.utils.data.DataLoader(Validationset, batch_size=opt.bs, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200ddec-922c-4d47-b6cb-4e0641755934",
   "metadata": {},
   "source": [
    "**Model setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e293ac-3991-4369-a614-7d38c7887bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1984\\218752443.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(path, 'Validation_model.t7'), map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "if opt.model == 'ResNet18':\n",
    "    net = ResNet18()\n",
    "elif opt.model == 'VGG19':\n",
    "    net = VGG('VGG19')\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    print(\"No checkpoint directory found. Starting training from scratch.\")\n",
    "    opt.resume = False\n",
    "\n",
    "if opt.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = torch.load(os.path.join(path, 'Validation_model.t7'), map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_Testing_acc = checkpoint['best_Testing_acc']\n",
    "    best_Validation_acc = checkpoint['best_Validation_acc']\n",
    "    best_Testing_acc_epoch = checkpoint['best_Testing_acc_epoch']\n",
    "    best_Validation_acc_epoch = checkpoint['best_Validation_acc_epoch']\n",
    "    start_epoch = checkpoint['best_Validation_acc_epoch'] + 1\n",
    "else:\n",
    "    print('==> Building model..')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d20e79-649f-4bf0-aa61-a709bc126ab5",
   "metadata": {},
   "source": [
    "**Traning Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4bbc9e-829a-4978-9fbd-2db8dd3ec38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    global Train_acc\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch > learning_rate_decay_start and learning_rate_decay_start >= 0:\n",
    "        frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n",
    "        decay_factor = learning_rate_decay_rate ** frac\n",
    "        current_lr = opt.lr * decay_factor\n",
    "        utils.set_lr(optimizer, current_lr)  # set the decayed rate\n",
    "    else:\n",
    "        current_lr = opt.lr\n",
    "    print('learning_rate: %s' % str(current_lr))\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        utils.clip_gradient(optimizer, 0.1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).sum().item()\n",
    "\n",
    "        utils.progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                           % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    Train_acc = 100.*correct/total\n",
    "    train_acc_list.append(Train_acc)\n",
    "    train_loss_list.append(train_loss / len(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb84cbe-aa94-4316-aa3e-b126bd7c98cc",
   "metadata": {},
   "source": [
    "**Testing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2f6417-ca3b-4ace-b6dd-ae1287835403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing(epoch):\n",
    "    global Testing_acc\n",
    "    global best_Testing_acc\n",
    "    global best_Testing_acc_epoch\n",
    "    net.eval()\n",
    "    Testing_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(Testingloader):\n",
    "        bs, ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs, targets = inputs, targets\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "            outputs_avg = outputs.view(bs, ncrops, -1).mean(1)  # avg over crops\n",
    "            loss = criterion(outputs_avg, targets)\n",
    "            Testing_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs_avg.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "\n",
    "        utils.progress_bar(batch_idx, len(Testingloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                           % (Testing_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    Testing_acc = 100.*correct/total\n",
    "    if Testing_acc > best_Testing_acc:\n",
    "        print('Saving..')\n",
    "        print(\"best_Testing_acc: %0.3f\" % Testing_acc)\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': Testing_acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "        torch.save(state, os.path.join(path, 'Testing_model.t7'))\n",
    "        best_Testing_acc = Testing_acc\n",
    "        best_Testing_acc_epoch = epoch\n",
    "\n",
    "    testing_acc_list.append(Testing_acc)\n",
    "    testing_loss_list.append(Testing_loss / len(Testingloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3bdaf-1b49-4ecd-8ce8-91a99f56c0f1",
   "metadata": {},
   "source": [
    "**Validation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f49034d-07be-49bb-9aeb-8789a47e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation(epoch):\n",
    "    global Validation_acc\n",
    "    global best_Validation_acc\n",
    "    global best_Validation_acc_epoch\n",
    "    net.eval()\n",
    "    Validation_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, (inputs, targets) in enumerate(Validationloader):\n",
    "            bs, ncrops, c, h, w = np.shape(inputs)\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "            inputs, targets = inputs, targets\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            outputs_avg = outputs.view(bs, ncrops, -1).mean(1)  # avg over crops\n",
    "            loss = criterion(outputs_avg, targets)\n",
    "            Validation_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs_avg.data, 1)\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "            total += targets.size(0)  # Update the total number of samples\n",
    "\n",
    "            utils.progress_bar(batch_idx, len(Validationloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                               % (Validation_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    # Save checkpoint if this is the best accuracy\n",
    "    Validation_acc = 100. * correct / total\n",
    "    if Validation_acc > best_Validation_acc:\n",
    "        print('Saving..')\n",
    "        print(\"best_Validation_acc: %0.3f\" % Validation_acc)\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'best_Testing_acc': best_Testing_acc,\n",
    "            'best_Validation_acc': Validation_acc,\n",
    "            'best_Testing_acc_epoch': best_Testing_acc_epoch,\n",
    "            'best_Validation_acc_epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)\n",
    "        torch.save(state, os.path.join(path, 'Validation_model.t7'))\n",
    "        best_Validation_acc = Validation_acc\n",
    "        best_Validation_acc_epoch = epoch\n",
    "\n",
    "    validation_acc_list.append(Validation_acc)\n",
    "    validation_loss_list.append(Validation_loss / len(Validationloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b2c56-c5e0-4727-854c-bf2fda523ea5",
   "metadata": {},
   "source": [
    "**Traning Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4a4a41-99d2-4373-a94f-8362f9a7df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 77\n",
      "learning_rate: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\FER-2013 Project\\fer2013_vgg_resnet\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>] | Loss: 0.151 | Acc: 94.646% (27172/28709)    225/225 \n",
      " [============================>.] | Loss: 1.230 | Acc: 69.936% (2510/3589)      29/29 \n",
      " [============================>.] | Loss: 1.230 | Acc: 69.936% (2510/3589)      29/29 \n",
      "\n",
      "Epoch: 78\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.157 | Acc: 94.552% (27145/28709)    225/225 \n",
      " [============================>.] | Loss: 1.358 | Acc: 67.623% (2427/3589)      29/29 \n",
      " [============================>.] | Loss: 1.358 | Acc: 67.623% (2427/3589)      29/29 \n",
      "\n",
      "Epoch: 79\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.159 | Acc: 94.389% (27098/28709)    225/225 \n",
      " [============================>.] | Loss: 1.193 | Acc: 69.908% (2509/3589)      29/29 \n",
      " [============================>.] | Loss: 1.193 | Acc: 69.908% (2509/3589)      29/29 \n",
      "\n",
      "Epoch: 80\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.153 | Acc: 94.646% (27172/28709)    225/225 \n",
      " [============================>.] | Loss: 1.207 | Acc: 69.908% (2509/3589)      29/29 \n",
      " [============================>.] | Loss: 1.207 | Acc: 69.908% (2509/3589)      29/29 \n",
      "\n",
      "Epoch: 81\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.149 | Acc: 94.765% (27206/28709)    225/225 \n",
      " [============================>.] | Loss: 1.217 | Acc: 70.716% (2538/3589)      29/29 \n",
      "Saving..\n",
      "best_Testing_acc: 70.716\n",
      " [============================>.] | Loss: 1.217 | Acc: 70.716% (2538/3589)      29/29 \n",
      "Saving..\n",
      "best_Validation_acc: 70.716\n",
      "\n",
      "Epoch: 82\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.149 | Acc: 94.789% (27213/28709)    225/225 \n",
      " [============================>.] | Loss: 1.269 | Acc: 71.580% (2569/3589)      29/29 \n",
      "Saving..\n",
      "best_Testing_acc: 71.580\n",
      " [============================>.] | Loss: 1.269 | Acc: 71.580% (2569/3589)      29/29 \n",
      "Saving..\n",
      "best_Validation_acc: 71.580\n",
      "\n",
      "Epoch: 83\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.147 | Acc: 94.782% (27211/28709)    225/225 \n",
      " [============================>.] | Loss: 1.320 | Acc: 70.410% (2527/3589)      29/29 \n",
      " [============================>.] | Loss: 1.320 | Acc: 70.410% (2527/3589)      29/29 \n",
      "\n",
      "Epoch: 84\n",
      "learning_rate: 0.01\n",
      " [=============================>] | Loss: 0.147 | Acc: 94.793% (27214/28709)    225/225 \n",
      " [============================>.] | Loss: 1.243 | Acc: 69.824% (2506/3589)      29/29 \n",
      " [============================>.] | Loss: 1.243 | Acc: 69.824% (2506/3589)      29/29 \n",
      "\n",
      "Epoch: 85\n",
      "learning_rate: 0.009000000000000001\n",
      " [=============================>] | Loss: 0.123 | Acc: 95.799% (27503/28709)    225/225 \n",
      " [============================>.] | Loss: 1.290 | Acc: 70.577% (2533/3589)      29/29 \n",
      " [============================>.] | Loss: 1.290 | Acc: 70.577% (2533/3589)      29/29 \n",
      "\n",
      "Epoch: 86\n",
      "learning_rate: 0.009000000000000001\n",
      " [=============================>] | Loss: 0.113 | Acc: 96.134% (27599/28709)    225/225 \n",
      " [============================>.] | Loss: 1.254 | Acc: 70.410% (2527/3589)      29/29 \n",
      " [============================>.] | Loss: 1.254 | Acc: 70.410% (2527/3589)      29/29 \n",
      "\n",
      "Epoch: 87\n",
      "learning_rate: 0.009000000000000001\n",
      " [=============================>] | Loss: 0.114 | Acc: 96.033% (27570/28709)    225/225 \n",
      " [============================>.] | Loss: 1.211 | Acc: 71.190% (2555/3589)      29/29 \n",
      " [============================>.] | Loss: 1.211 | Acc: 71.190% (2555/3589)      29/29 \n",
      "\n",
      "Epoch: 88\n",
      "learning_rate: 0.009000000000000001\n",
      " [=============================>] | Loss: 0.120 | Acc: 95.827% (27511/28709)    225/225 \n",
      " [============================>.] | Loss: 1.308 | Acc: 71.385% (2562/3589)      29/29 \n",
      " [============================>.] | Loss: 1.308 | Acc: 71.385% (2562/3589)      29/29 \n",
      "\n",
      "Epoch: 89\n",
      "learning_rate: 0.009000000000000001\n",
      " [=============================>] | Loss: 0.109 | Acc: 96.235% (27628/28709)    225/225 \n",
      " [============================>.] | Loss: 1.262 | Acc: 71.385% (2562/3589)      29/29 \n",
      " [============================>.] | Loss: 1.262 | Acc: 71.385% (2562/3589)      29/29 \n",
      "\n",
      "Epoch: 90\n",
      "learning_rate: 0.008100000000000001\n",
      " [=============================>] | Loss: 0.093 | Acc: 96.736% (27772/28709)    225/225 \n",
      " [============================>.] | Loss: 1.321 | Acc: 71.273% (2558/3589)      29/29 \n",
      " [============================>.] | Loss: 1.321 | Acc: 71.273% (2558/3589)      29/29 \n",
      "\n",
      "Epoch: 91\n",
      "learning_rate: 0.008100000000000001\n",
      " [=============================>] | Loss: 0.087 | Acc: 96.851% (27805/28709)    225/225 \n",
      " [============================>.] | Loss: 1.264 | Acc: 70.382% (2526/3589)      29/29 \n",
      " [============================>.] | Loss: 1.264 | Acc: 70.382% (2526/3589)      29/29 \n",
      "\n",
      "Epoch: 92\n",
      "learning_rate: 0.008100000000000001\n",
      " [=============================>] | Loss: 0.084 | Acc: 97.067% (27867/28709)    225/225 \n",
      " [============================>.] | Loss: 1.315 | Acc: 71.106% (2552/3589)      29/29 \n",
      " [============================>.] | Loss: 1.315 | Acc: 71.106% (2552/3589)      29/29 \n",
      "\n",
      "Epoch: 93\n",
      "learning_rate: 0.008100000000000001\n",
      " [=============================>] | Loss: 0.086 | Acc: 96.970% (27839/28709)    225/225 \n",
      " [============================>.] | Loss: 1.257 | Acc: 70.911% (2545/3589)      29/29 \n",
      " [============================>.] | Loss: 1.257 | Acc: 70.911% (2545/3589)      29/29 \n",
      "\n",
      "Epoch: 94\n",
      "learning_rate: 0.008100000000000001\n",
      " [=============================>] | Loss: 0.084 | Acc: 96.997% (27847/28709)    225/225 \n",
      " [============================>.] | Loss: 1.313 | Acc: 71.719% (2574/3589)      29/29 \n",
      "Saving..\n",
      "best_Testing_acc: 71.719\n",
      " [============================>.] | Loss: 1.313 | Acc: 71.719% (2574/3589)      29/29 \n",
      "Saving..\n",
      "best_Validation_acc: 71.719\n",
      "\n",
      "Epoch: 95\n",
      "learning_rate: 0.007290000000000001\n",
      " [=============================>] | Loss: 0.076 | Acc: 97.290% (27931/28709)    225/225 \n",
      " [============================>.] | Loss: 1.254 | Acc: 71.050% (2550/3589)      29/29 \n",
      " [============================>.] | Loss: 1.254 | Acc: 71.050% (2550/3589)      29/29 \n",
      "\n",
      "Epoch: 96\n",
      "learning_rate: 0.007290000000000001\n",
      " [=============================>] | Loss: 0.065 | Acc: 97.739% (28060/28709)    225/225 \n",
      " [============================>.] | Loss: 1.223 | Acc: 71.190% (2555/3589)      29/29 \n",
      " [============================>.] | Loss: 1.223 | Acc: 71.190% (2555/3589)      29/29 \n",
      "\n",
      "Epoch: 97\n",
      "learning_rate: 0.007290000000000001\n",
      " [=============================>] | Loss: 0.068 | Acc: 97.677% (28042/28709)    225/225 \n",
      " [============================>.] | Loss: 1.423 | Acc: 70.605% (2534/3589)      29/29 \n",
      " [============================>.] | Loss: 1.423 | Acc: 70.605% (2534/3589)      29/29 \n",
      "\n",
      "Epoch: 98\n",
      "learning_rate: 0.007290000000000001\n",
      " [=============================>] | Loss: 0.065 | Acc: 97.743% (28061/28709)    225/225 \n",
      " [============================>.] | Loss: 1.314 | Acc: 71.552% (2568/3589)      29/29 \n",
      " [============================>.] | Loss: 1.314 | Acc: 71.552% (2568/3589)      29/29 \n",
      "\n",
      "Epoch: 99\n",
      "learning_rate: 0.007290000000000001\n",
      " [=============================>] | Loss: 0.063 | Acc: 97.792% (28075/28709)    225/225 \n",
      " [============================>.] | Loss: 1.321 | Acc: 70.521% (2531/3589)      29/29 \n",
      " [============================>.] | Loss: 1.321 | Acc: 70.521% (2531/3589)      29/29 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, total_epoch):\n",
    "    train(epoch)\n",
    "    Testing(epoch)\n",
    "    Validation(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ef25cf-47b0-4a9b-91d6-139aa21caced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_Testing_acc: 71.719\n",
      "best_Testing_acc_epoch: 94\n",
      "best_Validation_acc: 71.719\n",
      "best_Validation_acc_epoch: 94\n"
     ]
    }
   ],
   "source": [
    "print(\"best_Testing_acc: %0.3f\" % best_Testing_acc)\n",
    "print(\"best_Testing_acc_epoch: %d\" % best_Testing_acc_epoch)\n",
    "print(\"best_Validation_acc: %0.3f\" % best_Validation_acc)\n",
    "print(\"best_Validation_acc_epoch: %d\" % best_Validation_acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259ff59a-02d3-4d7a-8b8c-f75729d6ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "metrics = {\n",
    "    'train_acc_list': train_acc_list,\n",
    "    'train_loss_list': train_loss_list,\n",
    "    'testing_acc_list': testing_acc_list,\n",
    "    'testing_loss_list': testing_loss_list,\n",
    "    'validation_acc_list': validation_acc_list,\n",
    "    'validation_loss_list': validation_loss_list\n",
    "}\n",
    "\n",
    "with open(os.path.join(path, 'metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f62962-e0c0-4ace-9983-7cfe1b46482d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Facial Expression Recognition",
   "language": "python",
   "name": "facialexpressionrecogination"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
